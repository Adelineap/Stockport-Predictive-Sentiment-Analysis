Overview
The stock market is a focus for investors to maximize their potential profits and consequently, the interest shown from the technical and financial sides in stock market prediction is always on the rise.

However, stock market prediction is a problem known for its challenging nature due to its dependency on diverse factors that affect the market, these factors are unpredictable and cannot be taken into consideration such as political variables, and social media effects such as twitter on the stock market.
Link for github repo for this project Github repo

Import important libraries
import numpy as np
import tensorflow as tf
from keras.callbacks import ModelCheckpoint
from keras.models import Sequential
from keras.layers import LSTM,Conv1D,Conv2D,MaxPooling2D,MaxPooling1D,Flatten
from keras.layers import Dense, Dropout
import pandas as pd
# from keras.optimizers import Adam
from matplotlib import pyplot as plt
from sklearn.preprocessing import StandardScaler,MinMaxScaler
import seaborn as sns
#from datetime import datetime

from tensorflow.compat.v1.keras.layers import CuDNNLSTM,Bidirectional
seed=42
Load the dataset from kaggle
#Read the csv file
df = pd.read_csv('../input/netflex-stock-dataset-with-twitter-sentiment/Final_nflx_data_2018-2022.csv')
df['date'] = pd.to_datetime(df['date'])
df
date	Open	High	Low	Close	Adj Close	Volume	P_mean	P_sum	twt_count
0	2018-01-02	196.100006	201.649994	195.419998	201.070007	201.070007	10966900	0.020833	10	480
1	2018-01-03	202.050003	206.210007	201.500000	205.050003	205.050003	8591400	0.071217	24	337
2	2018-01-04	206.199997	207.050003	204.000000	205.630005	205.630005	6029600	-0.018519	-4	216
3	2018-01-05	207.250000	210.020004	205.589996	209.990005	209.990005	7033200	-0.019737	-6	304
4	2018-01-08	210.020004	212.500000	208.440002	212.050003	212.050003	5580200	-0.007663	-2	261
...	...	...	...	...	...	...	...	...	...	...
1132	2022-07-01	176.490005	180.100006	174.270004	179.949997	179.949997	5194700	-0.062315	-21	337
1133	2022-07-05	176.279999	185.919998	172.679993	185.880005	185.880005	7334300	-0.058824	-25	425
1134	2022-07-06	185.199997	186.220001	180.820007	184.059998	184.059998	5753400	-0.014870	-8	538
1135	2022-07-07	184.270004	190.210007	183.500000	189.270004	189.270004	6334500	-0.055427	-24	433
1136	2022-07-08	186.020004	189.910004	182.750000	186.979996	186.979996	5831300	-0.043011	-12	279
1137 rows × 10 columns

Understanding the time Series
import seaborn as sns
plt.figure(figsize=(25,7));
sns.lineplot(x=df["date"],y=df["Adj Close"])
df['sentiment_analysis']=df['P_mean']
df['sentiment_analysis']=df['sentiment_analysis'].apply(lambda x: 'pos' if x>0 else 'nue' if x==0 else 'neg')
sns.scatterplot(x=df["date"],y=df['Adj Close'],hue=df['sentiment_analysis'],palette=['y','r','g'])
plt.xticks(rotation=45);
plt.title("Stock market of Netfilx from Jan-2018 to Jul-2022",fontsize=16);

df['sentiment_analysis'].value_counts()
neg    1059
pos      71
nue       7
Name: sentiment_analysis, dtype: int64
df=df.drop(list(range(14)),axis=0,inplace=False)
df
date	Open	High	Low	Close	Adj Close	Volume	P_mean	P_sum	twt_count	sentiment_analysis
14	2018-01-23	255.050003	257.709991	248.020004	250.289993	250.289993	27705300	-0.061196	-88	1438	neg
15	2018-01-24	250.880005	261.709991	249.309998	261.299988	261.299988	17352400	-0.084525	-65	769	neg
16	2018-01-25	263.000000	272.299988	260.230011	269.700012	269.700012	15336400	-0.122744	-102	831	neg
17	2018-01-26	271.489990	274.600006	268.760010	274.600006	274.600006	11021800	-0.118012	-57	483	neg
18	2018-01-29	274.200012	286.809998	273.920013	284.589996	284.589996	17529700	-0.100690	-73	725	neg
...	...	...	...	...	...	...	...	...	...	...	...
1132	2022-07-01	176.490005	180.100006	174.270004	179.949997	179.949997	5194700	-0.062315	-21	337	neg
1133	2022-07-05	176.279999	185.919998	172.679993	185.880005	185.880005	7334300	-0.058824	-25	425	neg
1134	2022-07-06	185.199997	186.220001	180.820007	184.059998	184.059998	5753400	-0.014870	-8	538	neg
1135	2022-07-07	184.270004	190.210007	183.500000	189.270004	189.270004	6334500	-0.055427	-24	433	neg
1136	2022-07-08	186.020004	189.910004	182.750000	186.979996	186.979996	5831300	-0.043011	-12	279	neg
1123 rows × 11 columns

train_dates = pd.to_datetime(df['date'])
# print(train_dates.tail(15)) #Check last few dates.

#Variables for training
cols = [
    'Open',
    'High', 'Low',
    'Close',
    'Volume',
    'Adj Close',
    'P_mean',
        ]
#Date and volume columns are not used in training.
print(cols)

#New dataframe with only training data - 5 columns
df_for_training = df[cols].astype(float)
df_for_training.index=df['date']
df_for_training
['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close', 'P_mean']
Open	High	Low	Close	Volume	Adj Close	P_mean
date							
2018-01-23	255.050003	257.709991	248.020004	250.289993	27705300.0	250.289993	-0.061196
2018-01-24	250.880005	261.709991	249.309998	261.299988	17352400.0	261.299988	-0.084525
2018-01-25	263.000000	272.299988	260.230011	269.700012	15336400.0	269.700012	-0.122744
2018-01-26	271.489990	274.600006	268.760010	274.600006	11021800.0	274.600006	-0.118012
2018-01-29	274.200012	286.809998	273.920013	284.589996	17529700.0	284.589996	-0.100690
...	...	...	...	...	...	...	...
2022-07-01	176.490005	180.100006	174.270004	179.949997	5194700.0	179.949997	-0.062315
2022-07-05	176.279999	185.919998	172.679993	185.880005	7334300.0	185.880005	-0.058824
2022-07-06	185.199997	186.220001	180.820007	184.059998	5753400.0	184.059998	-0.014870
2022-07-07	184.270004	190.210007	183.500000	189.270004	6334500.0	189.270004	-0.055427
2022-07-08	186.020004	189.910004	182.750000	186.979996	5831300.0	186.979996	-0.043011
1123 rows × 7 columns

LSTM Model
Data scaling for LSTM because uses sigmoid and tanh that are sensitive to magnitude
scaler = MinMaxScaler()
scaler = scaler.fit(df_for_training)
df_for_training_scaled = scaler.transform(df_for_training)

scaler_for_inference = MinMaxScaler()
scaler_for_inference.fit_transform(df_for_training.loc[:,['Open','Adj Close']])

df_for_training_scaled
# df_for_training_scaled=df_for_training.copy()
# df_for_training_scaled=df_for_training_scaled.to_numpy()
array([[0.17239161, 0.16193068, 0.16299819, ..., 0.20085146, 0.15975024,
        0.58996553],
       [0.16449971, 0.16949312, 0.16546293, ..., 0.12256481, 0.18070889,
        0.52005358],
       [0.18743731, 0.18951466, 0.18632733, ..., 0.10732021, 0.19669918,
        0.40552279],
       ...,
       [0.04019756, 0.02677103, 0.03460201, ..., 0.0348554 , 0.03367472,
        0.72879365],
       [0.03843751, 0.03431458, 0.03972256, ..., 0.03924957, 0.04359249,
        0.60725335],
       [0.04174946, 0.03374739, 0.03828956, ..., 0.03544446, 0.03923323,
        0.6444625 ]])
As required for LSTM networks, we require to reshape an input data into n_samples x timesteps x n_features.
#Empty lists to be populated using formatted training data
trainX = []
trainY = []

n_future = 1   # Number of days we want to look into the future based on the past days.
n_past = 5  # Number of past days we want to use to predict the future.

#Reformat input data into a shape: (n_samples x timesteps x n_features)
#In my example, my df_for_training_scaled has a shape (12823, 5)
#12823 refers to the number of data points and 5 refers to the columns (multi-variables).
for i in range(n_past, len(df_for_training_scaled) - n_future +1):
    trainX.append(df_for_training_scaled[i - n_past:i, 0:df_for_training.shape[1]])
    trainY.append(df_for_training_scaled[i + n_future - 1:i + n_future,[0,-2]])

trainX, trainY = np.array(trainX), np.array(trainY)

print('TrainX shape = {}.'.format(trainX.shape))
print('TrainY shape = {}.'.format(trainY.shape))
TrainX shape = (1118, 5, 7).
TrainY shape = (1118, 1, 2).
Train test split for LSTM
from sklearn.model_selection import train_test_split

X_train_lstm_without_twitter, X_test_lstm_without_twitter, y_train_lstm_without_twitter, y_test_lstm_without_twitter = train_test_split(trainX[:,:,:-1], trainY, test_size=0.2, shuffle=False)

X_train_lstm_twitter, X_test_lstm_twitter, y_train_lstm_twitter, y_test_lstm_twitter = train_test_split(trainX, trainY, test_size=0.2, shuffle=False)

X_train_lstm_without_twitter.shape,X_train_lstm_twitter.shape
((894, 5, 6), (894, 5, 7))
Train validation split for LSTM
from sklearn.model_selection import train_test_split

X_train_lstm_without_twitter, X_val_lstm_without_twitter, y_train_lstm_without_twitter, y_val_lstm_without_twitter = train_test_split(X_train_lstm_without_twitter, y_train_lstm_without_twitter, test_size=0.1, shuffle=False)

X_train_lstm_twitter, X_val_lstm_twitter, y_train_lstm_twitter, y_val_lstm_twitter = train_test_split(X_train_lstm_twitter, y_train_lstm_twitter, test_size=0.1, shuffle=False)

X_train_lstm_without_twitter.shape,X_train_lstm_twitter.shape
((804, 5, 6), (804, 5, 7))
Model architecture
def build_model(input_shape):
    tf.random.set_seed(seed)
    cnn_lstm_model = Sequential()

    cnn_lstm_model.add(Conv1D(filters=128, kernel_size=2, strides=1, padding='valid', input_shape=input_shape))
    cnn_lstm_model.add(MaxPooling1D(pool_size=2, strides=2))

    cnn_lstm_model.add(Conv1D(filters=64, kernel_size=2, strides=1, padding='valid'))
    cnn_lstm_model.add(MaxPooling1D(pool_size=1, strides=2))
    # cnn_lstm_model.add(MaxPooling1D(pool_size=1, strides=2))

    cnn_lstm_model.add(Bidirectional(LSTM(256, return_sequences=True)))
    cnn_lstm_model.add(Dropout(0.2))
    cnn_lstm_model.add(Bidirectional(LSTM(256, return_sequences=True)))
    cnn_lstm_model.add(Dropout(0.2))

    cnn_lstm_model.add(Dense(32, activation='relu'))


    cnn_lstm_model.add(Dense(trainY.shape[2], activation='relu'))

    # cnn_lstm_model.build(input_shape=(trainX.shape[0], trainX.shape[1], trainX.shape[2]))

    cnn_lstm_model.compile(optimizer='adam', loss='mse')
    cnn_lstm_model.summary()
    return cnn_lstm_model
# fit the model

cnn_lstm_model_without_twitter=build_model((X_train_lstm_without_twitter.shape[1],X_train_lstm_without_twitter.shape[2]))
cnn_lstm_model_twitter=build_model((X_train_lstm_twitter.shape[1],X_train_lstm_twitter.shape[2]))

history_without_twitter = cnn_lstm_model_without_twitter.fit(X_train_lstm_without_twitter, y_train_lstm_without_twitter, epochs=50, batch_size=64, validation_data=(X_val_lstm_without_twitter, y_val_lstm_without_twitter), verbose=1, )


history_twitter = cnn_lstm_model_twitter.fit(X_train_lstm_twitter, y_train_lstm_twitter, epochs=50, batch_size=64, validation_data=(X_val_lstm_twitter, y_val_lstm_twitter), verbose=1, )
2022-08-29 10:20:22.895224: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d (Conv1D)              (None, 4, 128)            1664      
_________________________________________________________________
max_pooling1d (MaxPooling1D) (None, 2, 128)            0         
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 1, 64)             16448     
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 1, 64)             0         
_________________________________________________________________
bidirectional (Bidirectional (None, 1, 512)            657408    
_________________________________________________________________
dropout (Dropout)            (None, 1, 512)            0         
_________________________________________________________________
bidirectional_1 (Bidirection (None, 1, 512)            1574912   
_________________________________________________________________
dropout_1 (Dropout)          (None, 1, 512)            0         
_________________________________________________________________
dense (Dense)                (None, 1, 32)             16416     
_________________________________________________________________
dense_1 (Dense)              (None, 1, 2)              66        
=================================================================
Total params: 2,266,914
Trainable params: 2,266,914
Non-trainable params: 0
_________________________________________________________________
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d_2 (Conv1D)            (None, 4, 128)            1920      
_________________________________________________________________
max_pooling1d_2 (MaxPooling1 (None, 2, 128)            0         
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 1, 64)             16448     
_________________________________________________________________
max_pooling1d_3 (MaxPooling1 (None, 1, 64)             0         
_________________________________________________________________
bidirectional_2 (Bidirection (None, 1, 512)            657408    
_________________________________________________________________
dropout_2 (Dropout)          (None, 1, 512)            0         
_________________________________________________________________
bidirectional_3 (Bidirection (None, 1, 512)            1574912   
_________________________________________________________________
dropout_3 (Dropout)          (None, 1, 512)            0         
_________________________________________________________________
dense_2 (Dense)              (None, 1, 32)             16416     
_________________________________________________________________
dense_3 (Dense)              (None, 1, 2)              66        
=================================================================
Total params: 2,267,170
Trainable params: 2,267,170
Non-trainable params: 0
_________________________________________________________________
Epoch 1/50
2022-08-29 10:20:25.269075: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
13/13 [==============================] - 11s 179ms/step - loss: 0.0709 - val_loss: 0.0491
Epoch 2/50
13/13 [==============================] - 0s 30ms/step - loss: 0.0103 - val_loss: 0.0013
Epoch 3/50
13/13 [==============================] - 0s 30ms/step - loss: 0.0034 - val_loss: 9.8307e-04
Epoch 4/50
13/13 [==============================] - 0s 31ms/step - loss: 0.0023 - val_loss: 3.2854e-04
Epoch 5/50
13/13 [==============================] - 0s 31ms/step - loss: 0.0014 - val_loss: 3.7527e-04
Epoch 6/50
13/13 [==============================] - 0s 30ms/step - loss: 0.0012 - val_loss: 3.4493e-04
Epoch 7/50
13/13 [==============================] - 0s 38ms/step - loss: 0.0011 - val_loss: 3.3904e-04
Epoch 8/50
13/13 [==============================] - 0s 31ms/step - loss: 0.0011 - val_loss: 3.2121e-04
Epoch 9/50
13/13 [==============================] - 0s 30ms/step - loss: 0.0011 - val_loss: 3.3591e-04
Epoch 10/50
13/13 [==============================] - 0s 30ms/step - loss: 0.0011 - val_loss: 3.3222e-04
Epoch 11/50
13/13 [==============================] - 0s 31ms/step - loss: 0.0010 - val_loss: 6.0713e-04
Epoch 12/50
13/13 [==============================] - 1s 96ms/step - loss: 0.0010 - val_loss: 4.2826e-04
Epoch 13/50
13/13 [==============================] - 0s 30ms/step - loss: 0.0011 - val_loss: 3.0753e-04
Epoch 14/50
13/13 [==============================] - 0s 31ms/step - loss: 9.5167e-04 - val_loss: 4.9900e-04
Epoch 15/50
13/13 [==============================] - 0s 30ms/step - loss: 9.8489e-04 - val_loss: 5.4505e-04
Epoch 16/50
13/13 [==============================] - 0s 30ms/step - loss: 9.5822e-04 - val_loss: 4.7943e-04
Epoch 17/50
13/13 [==============================] - 0s 31ms/step - loss: 0.0010 - val_loss: 2.9405e-04
Epoch 18/50
13/13 [==============================] - 0s 31ms/step - loss: 8.5604e-04 - val_loss: 3.5421e-04
Epoch 19/50
13/13 [==============================] - 0s 30ms/step - loss: 9.0404e-04 - val_loss: 3.0407e-04
Epoch 20/50
13/13 [==============================] - 0s 32ms/step - loss: 9.6653e-04 - val_loss: 9.1652e-04
Epoch 21/50
13/13 [==============================] - 0s 30ms/step - loss: 8.8714e-04 - val_loss: 5.9399e-04
Epoch 22/50
13/13 [==============================] - 0s 31ms/step - loss: 9.7332e-04 - val_loss: 6.0658e-04
Epoch 23/50
13/13 [==============================] - 0s 31ms/step - loss: 8.2952e-04 - val_loss: 4.0981e-04
Epoch 24/50
13/13 [==============================] - 0s 33ms/step - loss: 8.2337e-04 - val_loss: 2.4612e-04
Epoch 25/50
13/13 [==============================] - 0s 31ms/step - loss: 7.9487e-04 - val_loss: 2.7179e-04
Epoch 26/50
13/13 [==============================] - 0s 31ms/step - loss: 7.3754e-04 - val_loss: 2.2783e-04
Epoch 27/50
13/13 [==============================] - 0s 30ms/step - loss: 7.3675e-04 - val_loss: 2.2485e-04
Epoch 28/50
13/13 [==============================] - 0s 31ms/step - loss: 7.2842e-04 - val_loss: 0.0013
Epoch 29/50
13/13 [==============================] - 0s 30ms/step - loss: 8.9448e-04 - val_loss: 3.2885e-04
Epoch 30/50
13/13 [==============================] - 0s 30ms/step - loss: 7.9215e-04 - val_loss: 6.0040e-04
Epoch 31/50
13/13 [==============================] - 0s 30ms/step - loss: 6.3565e-04 - val_loss: 3.4793e-04
Epoch 32/50
13/13 [==============================] - 0s 30ms/step - loss: 6.7439e-04 - val_loss: 2.4479e-04
Epoch 33/50
13/13 [==============================] - 0s 36ms/step - loss: 7.3131e-04 - val_loss: 2.2531e-04
Epoch 34/50
13/13 [==============================] - 0s 30ms/step - loss: 6.6962e-04 - val_loss: 1.9734e-04
Epoch 35/50
13/13 [==============================] - 0s 31ms/step - loss: 6.2057e-04 - val_loss: 3.1328e-04
Epoch 36/50
13/13 [==============================] - 0s 30ms/step - loss: 6.5947e-04 - val_loss: 2.6362e-04
Epoch 37/50
13/13 [==============================] - 0s 30ms/step - loss: 7.2575e-04 - val_loss: 4.3566e-04
Epoch 38/50
13/13 [==============================] - 0s 31ms/step - loss: 6.8155e-04 - val_loss: 3.8557e-04
Epoch 39/50
13/13 [==============================] - 0s 33ms/step - loss: 6.6022e-04 - val_loss: 6.3383e-04
Epoch 40/50
13/13 [==============================] - 0s 30ms/step - loss: 6.8476e-04 - val_loss: 2.7395e-04
Epoch 41/50
13/13 [==============================] - 0s 33ms/step - loss: 5.8411e-04 - val_loss: 6.8913e-04
Epoch 42/50
13/13 [==============================] - 0s 30ms/step - loss: 6.3745e-04 - val_loss: 2.1446e-04
Epoch 43/50
13/13 [==============================] - 0s 31ms/step - loss: 6.9432e-04 - val_loss: 2.1560e-04
Epoch 44/50
13/13 [==============================] - 0s 30ms/step - loss: 7.6820e-04 - val_loss: 3.6786e-04
Epoch 45/50
13/13 [==============================] - 0s 31ms/step - loss: 6.7591e-04 - val_loss: 3.1036e-04
Epoch 46/50
13/13 [==============================] - 0s 30ms/step - loss: 5.7993e-04 - val_loss: 9.0617e-04
Epoch 47/50
13/13 [==============================] - 0s 30ms/step - loss: 7.4536e-04 - val_loss: 5.9262e-04
Epoch 48/50
13/13 [==============================] - 0s 31ms/step - loss: 7.4123e-04 - val_loss: 3.9611e-04
Epoch 49/50
13/13 [==============================] - 0s 30ms/step - loss: 6.5063e-04 - val_loss: 3.4164e-04
Epoch 50/50
13/13 [==============================] - 0s 31ms/step - loss: 6.4802e-04 - val_loss: 8.3358e-04
Epoch 1/50
13/13 [==============================] - 11s 176ms/step - loss: 0.0665 - val_loss: 0.0654
Epoch 2/50
13/13 [==============================] - 0s 31ms/step - loss: 0.0094 - val_loss: 0.0103
Epoch 3/50
13/13 [==============================] - 0s 30ms/step - loss: 0.0034 - val_loss: 5.3916e-04
Epoch 4/50
13/13 [==============================] - 0s 30ms/step - loss: 0.0018 - val_loss: 5.4929e-04
Epoch 5/50
13/13 [==============================] - 0s 31ms/step - loss: 0.0016 - val_loss: 5.6615e-04
Epoch 6/50
13/13 [==============================] - 0s 30ms/step - loss: 0.0014 - val_loss: 6.2545e-04
Epoch 7/50
13/13 [==============================] - 0s 30ms/step - loss: 0.0013 - val_loss: 4.7400e-04
Epoch 8/50
13/13 [==============================] - 0s 30ms/step - loss: 0.0012 - val_loss: 3.1937e-04
Epoch 9/50
13/13 [==============================] - 0s 32ms/step - loss: 0.0011 - val_loss: 2.9787e-04
Epoch 10/50
13/13 [==============================] - 0s 30ms/step - loss: 0.0011 - val_loss: 2.8418e-04
Epoch 11/50
13/13 [==============================] - 0s 30ms/step - loss: 0.0011 - val_loss: 5.5476e-04
Epoch 12/50
13/13 [==============================] - 0s 37ms/step - loss: 0.0010 - val_loss: 2.5385e-04
Epoch 13/50
13/13 [==============================] - 0s 31ms/step - loss: 0.0010 - val_loss: 2.3632e-04
Epoch 14/50
13/13 [==============================] - 0s 30ms/step - loss: 9.2271e-04 - val_loss: 2.4155e-04
Epoch 15/50
13/13 [==============================] - 0s 31ms/step - loss: 9.7189e-04 - val_loss: 4.0549e-04
Epoch 16/50
13/13 [==============================] - 0s 30ms/step - loss: 9.1340e-04 - val_loss: 9.1182e-04
Epoch 17/50
13/13 [==============================] - 0s 30ms/step - loss: 0.0011 - val_loss: 2.2127e-04
Epoch 18/50
13/13 [==============================] - 0s 31ms/step - loss: 9.0259e-04 - val_loss: 1.9988e-04
Epoch 19/50
13/13 [==============================] - 0s 32ms/step - loss: 8.4182e-04 - val_loss: 2.3752e-04
Epoch 20/50
13/13 [==============================] - 0s 30ms/step - loss: 8.9564e-04 - val_loss: 7.0227e-04
Epoch 21/50
13/13 [==============================] - 0s 30ms/step - loss: 7.6903e-04 - val_loss: 2.0692e-04
Epoch 22/50
13/13 [==============================] - 0s 31ms/step - loss: 9.5503e-04 - val_loss: 6.6779e-04
Epoch 23/50
13/13 [==============================] - 0s 31ms/step - loss: 7.8628e-04 - val_loss: 2.5274e-04
Epoch 24/50
13/13 [==============================] - 0s 30ms/step - loss: 7.9323e-04 - val_loss: 2.7169e-04
Epoch 25/50
13/13 [==============================] - 0s 31ms/step - loss: 7.3659e-04 - val_loss: 2.1003e-04
Epoch 26/50
13/13 [==============================] - 0s 30ms/step - loss: 7.2456e-04 - val_loss: 2.0252e-04
Epoch 27/50
13/13 [==============================] - 0s 31ms/step - loss: 6.9021e-04 - val_loss: 1.8969e-04
Epoch 28/50
13/13 [==============================] - 1s 43ms/step - loss: 6.9554e-04 - val_loss: 6.6433e-04
Epoch 29/50
13/13 [==============================] - 1s 77ms/step - loss: 8.6146e-04 - val_loss: 2.2071e-04
Epoch 30/50
13/13 [==============================] - 0s 31ms/step - loss: 7.7874e-04 - val_loss: 1.8501e-04
Epoch 31/50
13/13 [==============================] - 0s 31ms/step - loss: 6.2113e-04 - val_loss: 3.0023e-04
Epoch 32/50
13/13 [==============================] - 0s 30ms/step - loss: 6.4880e-04 - val_loss: 1.8833e-04
Epoch 33/50
13/13 [==============================] - 0s 31ms/step - loss: 6.8511e-04 - val_loss: 1.9048e-04
Epoch 34/50
13/13 [==============================] - 0s 30ms/step - loss: 6.5570e-04 - val_loss: 1.8903e-04
Epoch 35/50
13/13 [==============================] - 0s 33ms/step - loss: 5.7311e-04 - val_loss: 1.8472e-04
Epoch 36/50
13/13 [==============================] - 0s 30ms/step - loss: 5.7921e-04 - val_loss: 1.7661e-04
Epoch 37/50
13/13 [==============================] - 0s 38ms/step - loss: 6.6822e-04 - val_loss: 3.1378e-04
Epoch 38/50
13/13 [==============================] - 0s 34ms/step - loss: 6.0905e-04 - val_loss: 2.0801e-04
Epoch 39/50
13/13 [==============================] - 0s 32ms/step - loss: 6.7149e-04 - val_loss: 1.8982e-04
Epoch 40/50
13/13 [==============================] - 0s 31ms/step - loss: 6.1435e-04 - val_loss: 1.8527e-04
Epoch 41/50
13/13 [==============================] - 0s 31ms/step - loss: 6.0540e-04 - val_loss: 1.7570e-04
Epoch 42/50
13/13 [==============================] - 0s 32ms/step - loss: 5.8503e-04 - val_loss: 2.0510e-04
Epoch 43/50
13/13 [==============================] - 0s 30ms/step - loss: 6.0933e-04 - val_loss: 0.0013
Epoch 44/50
13/13 [==============================] - 0s 30ms/step - loss: 8.4047e-04 - val_loss: 3.1694e-04
Epoch 45/50
13/13 [==============================] - 0s 31ms/step - loss: 5.5872e-04 - val_loss: 1.8074e-04
Epoch 46/50
13/13 [==============================] - 0s 31ms/step - loss: 6.3536e-04 - val_loss: 7.2487e-04
Epoch 47/50
13/13 [==============================] - 0s 30ms/step - loss: 7.4021e-04 - val_loss: 8.7199e-04
Epoch 48/50
13/13 [==============================] - 0s 31ms/step - loss: 7.0756e-04 - val_loss: 2.6073e-04
Epoch 49/50
13/13 [==============================] - 0s 30ms/step - loss: 5.9418e-04 - val_loss: 2.6567e-04
Epoch 50/50
13/13 [==============================] - 0s 31ms/step - loss: 6.4069e-04 - val_loss: 5.4646e-04
Plotting Training and validation loss
plt.figure(figsize=(20,5))
plt.plot(history_without_twitter.history['loss'], label='Training loss')
plt.plot(history_without_twitter.history['val_loss'], label='Validation loss')
plt.title('Training loss Vs. Validation loss without twitter sentiment analysis')
plt.legend()

plt.figure(figsize=(20,5))
plt.plot(history_twitter.history['loss'], label='Training loss')
plt.plot(history_twitter.history['val_loss'], label='Validation loss')
plt.title('Training loss Vs. Validation loss including twitter sentiment analysis')
plt.legend()

Plotting
from sklearn.metrics import mean_squared_error,mean_absolute_error
from math import sqrt

def plot_predictions_with_dates (type,twitter,dates,y_actual_lstm,y_pred_lstm):
    predicted_features=['Open','Adj Close']
    for i,predicted_feature in enumerate(predicted_features):
        plt.figure(figsize=(15,6))
        if twitter :
            plt.title(f'LSTM {type} prediction of {predicted_feature} feature After adding twitter sentiment analysis')
        else:
            plt.title(f'LSTM {type} prediction of {predicted_feature} feature without twitter sentiment analysis')
        sns.lineplot(x=dates, y=y_actual_lstm[:,i],label='Actual')
        sns.lineplot(x=dates, y=y_pred_lstm[:, i], label='Predicted')
        plt.show()
        error=mean_squared_error(y_actual_lstm[:,i], y_pred_lstm[:, i])
        print(f'Mean square error for {predicted_feature} ={error}')
    print('Total mean square error', mean_squared_error(y_actual_lstm, y_pred_lstm))
Computing training accuracy
training_dates= df_for_training.index[:X_train_lstm_without_twitter.shape[0]]
#Make prediction
training_prediction_without_twitter = cnn_lstm_model_without_twitter.predict(X_train_lstm_without_twitter)

training_prediction_twitter = cnn_lstm_model_twitter.predict(X_train_lstm_twitter)

training_prediction_without_twitter=training_prediction_without_twitter.reshape(training_prediction_without_twitter.shape[0], training_prediction_without_twitter.shape[2])

training_prediction_twitter=training_prediction_twitter.reshape(training_prediction_twitter.shape[0], training_prediction_twitter.shape[2])

y_train_pred_lstm_without_twitter = scaler_for_inference.inverse_transform(training_prediction_without_twitter)

y_train_pred_lstm_twitter = scaler_for_inference.inverse_transform(training_prediction_twitter)

y_train_lstm_reshaped_without_twitter=y_train_lstm_without_twitter.reshape(y_train_lstm_without_twitter.shape[0], y_train_lstm_without_twitter.shape[2])

y_train_actual_lstm = scaler_for_inference.inverse_transform(y_train_lstm_reshaped_without_twitter)
Training accuracy without twitter
plot_predictions_with_dates('Training',False,training_dates,y_train_actual_lstm,y_train_pred_lstm_without_twitter)

Mean square error for Open =162.33441189827212

Mean square error for Adj Close =175.9916355376002
Total mean square error 169.16302371793614
